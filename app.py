import streamlit as st
import pandas as pd
import joblib
import os
import json
from datetime import datetime, timedelta
from openai import OpenAI
from prophet import Prophet
import plotly.express as px

# ==========================================
# 1. ç³»ç»Ÿé…ç½®
# ==========================================
st.set_page_config(page_title="CityWalk Pro å…¨å›½ç‰ˆ", page_icon="ğŸŒ", layout="wide")

# ==========================================
# 0. é…ç½®å¤§æ¨¡å‹ (ä½¿ç”¨ç¡…åŸºæµåŠ¨å…è´¹é¢åº¦)
# ==========================================
# å¡«å…¥ä½ åœ¨ç¡…åŸºæµåŠ¨ç”³è¯·çš„ Key (sk-å¼€å¤´)
API_KEY = "sk-jaabjqopkduryfbotghlprjmpsadfhszpzcfspnmamarpdhb"
BASE_URL = "https://api.siliconflow.cn/v1"

# æ³¨æ„ï¼šç¡…åŸºæµåŠ¨çš„ DeepSeek æ¨¡å‹åå­—æ¯”è¾ƒé•¿ï¼Œåˆ«å†™é”™
MODEL_NAME = "deepseek-ai/DeepSeek-V3"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)



# ==========================================
# 2. åŠ¨æ€åŠ è½½æœåŠ¡
# ==========================================
def get_available_cities():
    """æ‰«æ models æ–‡ä»¶å¤¹ä¸‹çš„åŸå¸‚åˆ—è¡¨"""
    base_dir = "city_models"
    if not os.path.exists(base_dir):
        return []
    # è·å–æ–‡ä»¶å¤¹åç§°ä½œä¸ºåŸå¸‚å
    cities = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
    return cities


@st.cache_resource
def load_city_models(city_name):
    """åªåŠ è½½é€‰ä¸­åŸå¸‚çš„æ¨¡å‹"""
    models = {}
    city_dir = os.path.join("city_models", city_name)
    if not os.path.exists(city_dir):
        return {}
    for filename in os.listdir(city_dir):
        if filename.endswith('.pkl'):
            spot_name = filename.replace('.pkl', '')
            try:
                models[spot_name] = joblib.load(os.path.join(city_dir, filename))
            except:
                pass
    return models


def predict_city_traffic(city_models, target_time):
    """é¢„æµ‹è¯¥åŸå¸‚æ‰€æœ‰æ™¯ç‚¹çš„å®¢æµ"""
    results = []
    future_df = pd.DataFrame({'ds': [target_time]})

    for spot, model in city_models.items():
        forecast = model.predict(future_df)
        flow = int(forecast['yhat'].values[0])
        flow = max(0, flow)

        # --- ä¿®å¤ 1: è°ƒæ•´é˜ˆå€¼ï¼Œè®©é¢œè‰²æ›´ä¸°å¯Œ ---
        # ä¹‹å‰çš„é˜ˆå€¼å¤ªé«˜äº†ï¼Œå¯¼è‡´å…¨æ˜¯ç»¿è‰²ã€‚ç°åœ¨è°ƒä½ä¸€ç‚¹ã€‚
        if flow < 200:
            status = "èˆ’é€‚ ğŸŸ¢"
            color_val = "green"
        elif flow < 400:
            status = "é€‚ä¸­ ğŸŸ¡"
            color_val = "yellow"
        else:
            status = "æ‹¥æŒ¤ ğŸ”´"
            color_val = "red"

        results.append({
            "æ™¯ç‚¹": spot,
            "é¢„è®¡å®¢æµ": flow,
            "çŠ¶æ€": status,
            "Color": color_val  # ç”¨äºæ’åºæˆ–ç»˜å›¾
        })

    if not results:
        return pd.DataFrame(columns=["æ™¯ç‚¹", "é¢„è®¡å®¢æµ", "çŠ¶æ€"])

    # æŒ‰å®¢æµä»é«˜åˆ°ä½æ’åº
    return pd.DataFrame(results).sort_values(by="é¢„è®¡å®¢æµ", ascending=False)


def call_llm_rag(user_query, city_name, traffic_data):
    """RAG: æ³¨å…¥å½“å‰åŸå¸‚çš„å®¢æµæ•°æ®"""
    # ç®€åŒ–ä¸Šä¸‹æ–‡ï¼Œåªä¼ å‰10ä¸ªï¼Œé˜²æ­¢Tokenè¿‡å¤š
    data_context = traffic_data[['æ™¯ç‚¹', 'é¢„è®¡å®¢æµ', 'çŠ¶æ€']].head(10).to_string(index=False)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¯¼æ¸¸ï¼Œå½“å‰åŸå¸‚ï¼šã€{city_name}ã€‘ã€‚

    ã€å®æ—¶å®¢æµæ•°æ®ã€‘
    {data_context}

    ã€ä»»åŠ¡ã€‘
    1. ä¸ºç”¨æˆ·è§„åˆ’è·¯çº¿ï¼Œå¿…é¡»åŸºäºæ•°æ®ã€‚
    2. ä¼˜å…ˆæ¨èâ€œèˆ’é€‚ ğŸŸ¢â€çš„æ™¯ç‚¹ï¼Œé¿å¼€â€œæ‹¥æŒ¤ ğŸ”´â€ã€‚
    3. è¾“å‡ºæ ¼å¼æ¸…æ™°ï¼Œå¯ä»¥ä½¿ç”¨ Markdown åˆ—è¡¨ã€‚
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            stream=True
        )
        return response
    except Exception as e:
        return str(e)


# ==========================================
# 3. å‰ç«¯ç•Œé¢ (ä¿®å¤ç‰ˆ - ç§»é™¤ä¸ç¨³å®š HTML)
# ==========================================
# CSS ä»…ç”¨äºç¾åŒ–åŸç”Ÿç»„ä»¶ï¼Œä¸æ”¹å˜ç»“æ„
st.markdown("""
<style>
    /* è°ƒæ•´ä¾§è¾¹æ èƒŒæ™¯ */
    section[data-testid="stSidebar"] {
        background-color: #f8f9fa;
    }
    /* éšè—é¡µè„š */
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# --- ä¾§è¾¹æ ï¼šåŸå¸‚ä¸ç›‘æ§ ---
with st.sidebar:
    st.title("ğŸŒ åŸå¸‚æŒ‡æŒ¥ä¸­å¿ƒ")

    available_cities = get_available_cities()
    if not available_cities:
        st.error("âŒ è¯·å…ˆè¿è¡Œ data_engine.py ç”Ÿæˆæ•°æ®")
        current_city = None
    else:
        # é»˜è®¤é€‰æ·±åœ³
        default_idx = available_cities.index("æ·±åœ³") if "æ·±åœ³" in available_cities else 0
        current_city = st.selectbox("ğŸ“ åˆ‡æ¢åŸå¸‚", available_cities, index=default_idx)

    st.divider()

    # å®æ—¶ç›‘æ§å›¾è¡¨
    if current_city:
        st.markdown(f"### ğŸ“Š {current_city}å®æ—¶çƒ­åŠ›")
        city_models = load_city_models(current_city)

        if city_models:
            df_traffic = predict_city_traffic(city_models, datetime.now())

            # é¢œè‰²æ˜ å°„
            color_map = {
                "èˆ’é€‚ ğŸŸ¢": "#2ecc71",  # ç»¿
                "é€‚ä¸­ ğŸŸ¡": "#f1c40f",  # é»„
                "æ‹¥æŒ¤ ğŸ”´": "#e74c3c"  # çº¢
            }

            # ä½¿ç”¨ Plotly ç”»å›¾
            fig = px.bar(
                df_traffic,
                x='é¢„è®¡å®¢æµ',
                y='æ™¯ç‚¹',
                orientation='h',
                color='çŠ¶æ€',
                color_discrete_map=color_map,
                text='é¢„è®¡å®¢æµ',
                height=500
            )
            fig.update_layout(
                xaxis_title="",
                yaxis_title="",
                showlegend=False,
                yaxis={'categoryorder': 'total ascending'}  # è‡ªåŠ¨æ’åº
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æ•°æ®åŠ è½½ä¸­...")

# --- ä¸»ç•Œé¢ï¼šèŠå¤©åŒº (ä½¿ç”¨åŸç”Ÿç»„ä»¶ï¼Œä¸å†æŠ¥é”™) ---
if current_city:
    st.title(f"ğŸš€ CityWalk Pro Â· {current_city}ç«™")
    st.caption("åŸºäºè¿è¥å•†æ ¸å¿ƒæ•°æ® | RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ")

    # åˆå§‹åŒ–å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # åˆ‡æ¢åŸå¸‚æ—¶æ¸…ç©ºå†å²ï¼Œé˜²æ­¢ä¸²å°
    if "last_city" not in st.session_state or st.session_state.last_city != current_city:
        st.session_state.messages = []
        st.session_state.messages.append({
            "role": "assistant",
            "content": f"ğŸ‘‹ æ¬¢è¿æ¥åˆ° **{current_city}**ï¼\n\næˆ‘æ˜¯ä½ çš„ AI ä¼´æ¸¸ï¼Œæˆ‘å·²ç»è·å–äº†å…¨åŸæ™¯ç‚¹çš„å®æ—¶å®¢æµæ•°æ®ã€‚\nä½ å¯ä»¥é—®æˆ‘ï¼š\n- *â€œå¸®æˆ‘è§„åˆ’ä¸€æ¡äººå°‘çš„åŠæ—¥æ¸¸è·¯çº¿â€*\n- *â€œç°åœ¨å»å“ªé‡Œç©æ¯”è¾ƒèˆ’æœï¼Ÿâ€*"
        })
        st.session_state.last_city = current_city

    # 1. æ¸²æŸ“å†å²æ¶ˆæ¯ (ä½¿ç”¨ st.chat_message ç¨³å®šæ€§ MAX)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # 2. å¤„ç†æ–°è¾“å…¥
    if prompt := st.chat_input(f"åœ¨ {current_city} æ€ä¹ˆç©ï¼Ÿ"):
        # ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI å›å¤
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # è°ƒç”¨ LLM
            stream = call_llm_rag(prompt, current_city, df_traffic)

            if isinstance(stream, str):
                st.error(f"å‡ºé”™å•¦: {stream}")
            else:
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

        # å­˜å…¥å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§é€‰æ‹©åŸå¸‚")