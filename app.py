import streamlit as st
import pandas as pd
import joblib
import os
import json
from datetime import datetime, timedelta
from openai import OpenAI
from prophet import Prophet
import plotly.express as px

# ==========================================
# 1. ç³»ç»Ÿé…ç½®
# ==========================================
st.set_page_config(page_title="CityWalk Pro å…¨å›½ç‰ˆ", page_icon="ğŸŒ", layout="wide")

# ==========================================
# 0. é…ç½®å¤§æ¨¡å‹ (ä½¿ç”¨ç¡…åŸºæµåŠ¨å…è´¹é¢åº¦)
# ==========================================
# å¡«å…¥ä½ åœ¨ç¡…åŸºæµåŠ¨ç”³è¯·çš„ Key (sk-å¼€å¤´)
API_KEY = "sk-jaabjqopkduryfbotghlprjmpsadfhszpzcfspnmamarpdhb"
BASE_URL = "https://api.siliconflow.cn/v1"

# æ³¨æ„ï¼šç¡…åŸºæµåŠ¨çš„ DeepSeek æ¨¡å‹åå­—æ¯”è¾ƒé•¿ï¼Œåˆ«å†™é”™
MODEL_NAME = "deepseek-ai/DeepSeek-V3"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)



# ==========================================
# 2. åŠ¨æ€åŠ è½½æœåŠ¡
# ==========================================
def get_available_cities():
    """æ‰«æ models æ–‡ä»¶å¤¹ä¸‹çš„åŸå¸‚åˆ—è¡¨"""
    base_dir = "city_models"
    if not os.path.exists(base_dir):
        return []
    # è·å–æ–‡ä»¶å¤¹åç§°ä½œä¸ºåŸå¸‚å
    cities = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]
    return cities


# ä½¿ç”¨ cache æå‡æ€§èƒ½ï¼Œå½“ city æ”¹å˜æ—¶é‡æ–°åŠ è½½
@st.cache_resource
def load_city_models(city_name):
    """åªåŠ è½½é€‰ä¸­åŸå¸‚çš„æ¨¡å‹"""
    models = {}
    city_dir = os.path.join("city_models", city_name)

    if not os.path.exists(city_dir):
        return {}

    for filename in os.listdir(city_dir):
        if filename.endswith('.pkl'):
            spot_name = filename.replace('.pkl', '')
            try:
                models[spot_name] = joblib.load(os.path.join(city_dir, filename))
            except:
                pass
    return models


def predict_city_traffic(city_models, target_time):
    """é¢„æµ‹è¯¥åŸå¸‚æ‰€æœ‰æ™¯ç‚¹çš„å®¢æµ"""
    results = []
    future_df = pd.DataFrame({'ds': [target_time]})

    for spot, model in city_models.items():
        forecast = model.predict(future_df)
        flow = int(forecast['yhat'].values[0])
        flow = max(0, flow)

        if flow < 400:
            status = "èˆ’é€‚ ğŸŸ¢"
        elif flow < 800:
            status = "é€‚ä¸­ ğŸŸ¡"
        else:
            status = "æ‹¥æŒ¤ ğŸ”´"

        results.append({"æ™¯ç‚¹": spot, "é¢„è®¡å®¢æµ": flow, "çŠ¶æ€": status})

    if not results:
        return pd.DataFrame(columns=["æ™¯ç‚¹", "é¢„è®¡å®¢æµ", "çŠ¶æ€"])

    return pd.DataFrame(results).sort_values(by="é¢„è®¡å®¢æµ")


def call_llm_rag(user_query, city_name, traffic_data):
    """RAG: æ³¨å…¥å½“å‰åŸå¸‚çš„å®¢æµæ•°æ®"""
    data_context = traffic_data.to_string(index=False)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å¯¼æ¸¸ï¼Œå½“å‰ç”¨æˆ·æ‰€åœ¨çš„åŸå¸‚æ˜¯ã€{city_name}ã€‘ã€‚

    ã€è¯¥åŸå¸‚å®æ—¶å®¢æµç›‘æµ‹ã€‘
    {data_context}

    ã€ä»»åŠ¡ã€‘
    1. æ ¹æ®å®¢æµæ•°æ®ï¼Œä¸ºç”¨æˆ·è§„åˆ’åœ¨ã€{city_name}ã€‘çš„æ¸¸ç©è·¯çº¿ã€‚
    2. å¿…é¡»ä¼˜å…ˆæ¨èâ€œèˆ’é€‚â€çŠ¶æ€çš„æ™¯ç‚¹ã€‚
    3. å¦‚æœç”¨æˆ·é—®åˆ°å…¶ä»–åŸå¸‚ï¼Œè¯·ç¤¼è²Œæé†’å…ˆåˆ‡æ¢åŸå¸‚ã€‚
    4. ç»“åˆè¯¥åŸå¸‚çš„æ–‡åŒ–ç‰¹è‰²ï¼ˆå¦‚è¥¿å®‰çš„å†å²ã€é‡åº†çš„é­”å¹»ï¼‰è¿›è¡Œè®²è§£ã€‚
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            stream=True
        )
        return response
    except Exception as e:
        return f"AI æœåŠ¡å¼‚å¸¸: {e}"


# ==========================================
# 3. å‰ç«¯ç•Œé¢
# ==========================================
# CSS ç¾åŒ–
st.markdown("""
<style>
    .stApp {background-color: #ffffff;}
    .css-1d391kg {padding-top: 1rem;} 
    /* ä¾§è¾¹æ ä¼˜åŒ– */
    section[data-testid="stSidebar"] {
        background-color: #f7f9fc;
        border-right: 1px solid #e3e6f0;
    }
</style>
""", unsafe_allow_html=True)

# --- ä¾§è¾¹æ ï¼šåŸå¸‚é€‰æ‹©ä¸ç›‘æ§ ---
with st.sidebar:
    st.title("ğŸŒ åŸå¸‚æŒ‡æŒ¥ä¸­å¿ƒ")

    # 1. åŸå¸‚é€‰æ‹©å™¨
    available_cities = get_available_cities()
    if not available_cities:
        st.error("âŒ æœªæ‰¾åˆ°æ•°æ®ï¼Œè¯·è¿è¡Œ data_engine.py")
        current_city = None
    else:
        # é»˜è®¤é€‰åŒ—äº¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™é€‰ç¬¬ä¸€ä¸ª
        default_idx = available_cities.index("åŒ—äº¬") if "åŒ—äº¬" in available_cities else 0
        current_city = st.selectbox("ğŸ“ å½“å‰åŸå¸‚", available_cities, index=default_idx)

    st.divider()

    # 2. å®æ—¶ç›‘æ§
    if current_city:
        st.markdown(f"### ğŸ“Š {current_city}å®æ—¶çƒ­åŠ›")
        target_time = datetime.now()  # é»˜è®¤ä¸ºå½“å‰æ—¶é—´

        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
        city_models = load_city_models(current_city)
        if city_models:
            df_traffic = predict_city_traffic(city_models, target_time)

            # å±•ç¤ºå›¾è¡¨
            fig = px.bar(df_traffic, x='é¢„è®¡å®¢æµ', y='æ™¯ç‚¹', orientation='h',
                         color='çŠ¶æ€',
                         color_discrete_map={"èˆ’é€‚ ğŸŸ¢": "#2ecc71", "é€‚ä¸­ ğŸŸ¡": "#f1c40f", "æ‹¥æŒ¤ ğŸ”´": "#e74c3c"},
                         height=400)
            fig.update_layout(xaxis_title="", yaxis_title="", showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

            # å±•ç¤ºè¯¦ç»†æ•°æ®è¡¨
            with st.expander("æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
                st.dataframe(df_traffic, hide_index=True)
        else:
            st.warning("æ¨¡å‹åŠ è½½ä¸­...")

# --- ä¸»ç•Œé¢ ---
if current_city:
    st.title(f"ğŸš€ CityWalk Pro Â· {current_city}ç«™")
    st.caption(f"åŸºäºè¿è¥å•†æ ¸å¿ƒæ•°æ® | è¦†ç›–å…¨å›½ {len(available_cities)} ä¸ªçƒ­é—¨åŸå¸‚")

    # åˆå§‹åŒ–å†å²è®°å½• (åˆ‡æ¢åŸå¸‚æ—¶æ¸…ç©ºå†å²ï¼Œé¿å…ä¸Šä¸‹æ–‡æ··ä¹±)
    if "last_city" not in st.session_state or st.session_state.last_city != current_city:
        st.session_state.messages = [{"role": "assistant",
                                      "content": f"æ¬¢è¿æ¥åˆ°{current_city}ï¼æ‚¨å¯ä»¥é—®æˆ‘ï¼š\n\nâ€œ{current_city}æœ‰å“ªäº›äººå°‘å¥½ç©çš„åœ°æ–¹ï¼Ÿâ€\nâ€œå¸®æˆ‘è§„åˆ’ä¸€æ¡{current_city}çš„åŠæ—¥æ¸¸è·¯çº¿ã€‚â€"}]
        st.session_state.last_city = current_city

    # æ¸²æŸ“èŠå¤©
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # è¾“å…¥æ¡†
    if prompt := st.chat_input(f"é—®é—® {current_city} æ€ä¹ˆç©..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_resp = ""

            # ä¼ å…¥åŸå¸‚åå’Œæ•°æ®
            stream = call_llm_rag(prompt, current_city, df_traffic)

            if isinstance(stream, str):
                placeholder.error(stream)
            else:
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_resp += chunk.choices[0].delta.content
                        placeholder.markdown(full_resp + "â–Œ")
                placeholder.markdown(full_resp)

        st.session_state.messages.append({"role": "assistant", "content": full_resp})
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©ä¸€ä¸ªåŸå¸‚å¼€å§‹")